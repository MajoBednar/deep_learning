{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:01.838397300Z",
     "start_time": "2025-05-30T23:29:01.796084800Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import decimate\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "VERBOSE = 0  # constant for debugging, higher number means more printing\n",
    "TRAIN_VAL_SPLIT = 6\n",
    "DOWNSAMPLING_FACTOR = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:20.018314200Z",
     "start_time": "2025-05-30T23:29:19.985121200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def log(message: tuple, verbose_true: int, verbose_min):\n",
    "    \"\"\"Helper print function, based on verbose params.\"\"\"\n",
    "    if verbose_true >= verbose_min:\n",
    "        for entry in message:\n",
    "            print(entry, end=' ')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:02.671862200Z",
     "start_time": "2025-05-30T23:29:02.640076500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"Load data from .h5 file.\"\"\"\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        data = f[next(iter(f.keys()))][()]\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:03.338242400Z",
     "start_time": "2025-05-30T23:29:03.322617800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def data_loader(input_dir: str) -> tuple[np.ndarray, str]:\n",
    "    \"\"\"Generator for loading data and its filename from all .h5 files in a directory.\"\"\"\n",
    "    for file in os.listdir(input_dir):\n",
    "        if not file.endswith('.h5'):\n",
    "            continue\n",
    "        path = os.path.join(input_dir, file)\n",
    "        data = load_data(path)\n",
    "        yield data, file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:04.011877600Z",
     "start_time": "2025-05-30T23:29:03.970825300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def create_data_file(data, output_dir, filename: str):\n",
    "    \"\"\"Create .h5 file with given data.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        f.create_dataset(filename.rsplit('.', 1)[0], data=data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:04.686165200Z",
     "start_time": "2025-05-30T23:29:04.576132900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def get_file_info(filename: str) -> tuple[str, int]:\n",
    "    filename = filename.rsplit('.', 1)[0].split('_')\n",
    "    task, _, window = filename[-3], filename[-2], filename[-1]\n",
    "    return task, int(window)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:05.266118700Z",
     "start_time": "2025-05-30T23:29:05.218234500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def downsample(data, factor):\n",
    "    \"\"\"Downsample data based on factor.\"\"\"\n",
    "    return decimate(data, factor, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:07.243383600Z",
     "start_time": "2025-05-30T23:29:07.227385900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def get_means(data) -> np.ndarray:\n",
    "    \"\"\"Get means for every sensor dimension.\"\"\"\n",
    "    return np.mean(data, axis=1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:07.880124800Z",
     "start_time": "2025-05-30T23:29:07.868945500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "def get_stds(data) -> np.ndarray:\n",
    "    \"\"\"Get standard deviations for every sensor dimension.\"\"\"\n",
    "    return np.std(data, axis=1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:08.439069200Z",
     "start_time": "2025-05-30T23:29:08.416219900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def z_score_normalize(data, means, stds):\n",
    "    \"\"\"Z-score normalization for every sensor dimension.\"\"\"\n",
    "    return (data - means) / stds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:09.315063900Z",
     "start_time": "2025-05-30T23:29:09.292424700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def downsample_and_save(input_dir, output_dir, factor, verbose):\n",
    "    \"\"\"Downsample data in each file and save them in a new file.\"\"\"\n",
    "    for data, file in data_loader(input_dir):\n",
    "        data = downsample(data, factor)\n",
    "        create_data_file(data, output_dir, f\"ds_{file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:09.972355400Z",
     "start_time": "2025-05-30T23:29:09.941573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def normalize_and_save(input_dir, output_dir, means, stds, verbose):\n",
    "    \"\"\"Normalize data in each file and save them in a new file.\"\"\"\n",
    "    for data, file in data_loader(input_dir):\n",
    "        data = z_score_normalize(data, means, stds)\n",
    "        create_data_file(data, output_dir, f\"norm_{file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:15.295405100Z",
     "start_time": "2025-05-30T23:29:15.267516700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def concat_together(input_dir):\n",
    "    \"\"\"\n",
    "    Concatenate data from files into a single time window of sensor readings.\n",
    "    Also split to a train and validation sets.\n",
    "    \"\"\"\n",
    "    train_data = None\n",
    "    val_data = None\n",
    "    for data, file in data_loader(input_dir):\n",
    "        _, window = get_file_info(file)\n",
    "        if window <= TRAIN_VAL_SPLIT and train_data is None:\n",
    "            train_data = data\n",
    "        elif window <= TRAIN_VAL_SPLIT:\n",
    "            train_data = np.concat((train_data, data), axis=1)\n",
    "        elif val_data is None:\n",
    "            val_data = data\n",
    "        else:\n",
    "            val_data = np.concat((train_data, data), axis=1)\n",
    "    return train_data, val_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:17.924423500Z",
     "start_time": "2025-05-30T23:29:17.905712700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:38.220778300Z",
     "start_time": "2025-05-30T23:29:20.711239300Z"
    }
   },
   "outputs": [],
   "source": [
    "downsample_and_save('Intra/train', 'Intra/train_ds', DOWNSAMPLING_FACTOR, VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "data_train, _ = concat_together('Intra/train_ds')\n",
    "means, stds = get_means(data_train), get_stds(data_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:29:44.012925100Z",
     "start_time": "2025-05-30T23:29:40.250157300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "normalize_and_save('Intra/train_ds', 'Intra/train_ds_norm', means, stds, VERBOSE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:30:22.452224200Z",
     "start_time": "2025-05-30T23:30:04.555661500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def concat_as_samples(input_dir):\n",
    "    \"\"\"\n",
    "    Concatenate data from files, treating each data file as a window sample\n",
    "    of sensor readings. Also split them to train and validation sets.\n",
    "    \"\"\"\n",
    "    samples_train = []\n",
    "    samples_val = []\n",
    "    for data, file in data_loader(input_dir):\n",
    "        _, window = get_file_info(file)\n",
    "        if window <= TRAIN_VAL_SPLIT:\n",
    "            samples_train.append(data)\n",
    "        else:\n",
    "            samples_val.append(data)\n",
    "    return np.stack(samples_train), np.stack(samples_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:32:08.790875500Z",
     "start_time": "2025-05-30T23:32:08.754489300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 248, 3563)\n",
      "(8, 248, 3563)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val = concat_as_samples('Intra/train_ds_norm')\n",
    "print(np.shape(data_train))\n",
    "print(np.shape(data_val))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T23:32:13.121823900Z",
     "start_time": "2025-05-30T23:32:11.020859600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T22:22:27.081592800Z",
     "start_time": "2025-05-30T22:22:26.407219900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
